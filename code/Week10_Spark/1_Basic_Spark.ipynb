{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCHHw7JmBE-R"
   },
   "source": [
    "# Spark Preparation\n",
    "We check if we are in Google Colab.  If this is the case, install all necessary packages.\n",
    "\n",
    "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.3.2 with hadoop 3.2, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab.\n",
    "Learn more from [A Must-Read Guide on How to Work with PySpark on Google Colab for Data Scientists!](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o5IklZSTX4mG"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9XiZNubiX9nq"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "    !wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "    !tar xf spark-3.3.2-bin-hadoop3.tgz\n",
    "    !mv spark-3.3.2-bin-hadoop3 spark\n",
    "    !pip install -q findspark\n",
    "    import os\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    os.environ[\"SPARK_HOME\"] = \"/content/spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvbv_avAY5b_"
   },
   "source": [
    "# Start a Local Cluster\n",
    "Use findspark.init() to start a local cluster.  If you plan to use remote cluster, skip the findspark.init() and change the cluster_url according."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PQg0Ed6cOl5b"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b715cKafZNkF"
   },
   "outputs": [],
   "source": [
    "spark_url = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VuvWws-GCpMg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 10:56:29 WARN Utils: Your hostname, Natawuts-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.203.226.183 instead (on interface en0)\n",
      "24/03/01 10:56:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/01 10:56:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(spark_url)\\\n",
    "        .appName('Spark Tutorial')\\\n",
    "        .config('spark.ui.port', '4040')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4KY8J5jBB3e"
   },
   "source": [
    "# Spark Entry Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ytNE6TgaBB3j"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.203.226.183:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x113debcd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.203.226.183:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Spark Tutorial>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2USEaXdZBB3k"
   },
   "source": [
    "## Simple RDD Operations\n",
    "\n",
    "There are 2 types of RDD operations, tranformation and action.  Transformation is an operation applied on a RDD to create new RDD (or create a new RDD from data).  Action is an operation applied on a RDD to perform computation and send the result back to driver.\n",
    "\n",
    "### Transformation Operations\n",
    "- *sc.parallelize(data)* \n",
    "create an RDD from data\n",
    "- *rdd.filter(func)* \n",
    "create a new rdd from existing rdd and keep only those elements that func is true\n",
    "\n",
    "### Action Operations\n",
    "- *rdd.count()* \n",
    "count number of elements in an rdd\n",
    "- *rdd.first()* \n",
    "get the frist element in the rdd\n",
    "- *rdd.collect()* \n",
    "gather all elements in the rdd into a python list\n",
    "- *rdd.take(n)* \n",
    "gather first n-th elements in the rdd into a python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8_iMIj_iBB3k"
   },
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8_iMIj_iBB3k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n = rdd.count()\n",
    "print('count = {0}'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8_iMIj_iBB3k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "l = rdd.collect()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eFBOt2ZpBB3l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "l = rdd.take(3)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_rdd = rdd.filter(lambda d: d > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEh31kMTBB3l"
   },
   "source": [
    "## RDD Operations - map and reduce\n",
    "\n",
    "- *rdd.map(func)* -- **transformation** --\n",
    "create a new rdd by performing function func on each element in an rdd\n",
    "- *rdd.reduce(func)* -- **action** --\n",
    "aggregate all elements in an rdd using function func\n",
    "\n",
    "These two operations perform functions on rdd elements.  The function can be provided using lambda function.\n",
    "We can supply any lambda function to map and reduce operations.  For map operation, the function must take one input and return one output.  For reduce operation, the function must take two inputs and return one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DCvOjO9bBB3m"
   },
   "outputs": [],
   "source": [
    "data = ['line 1', '2', 'more lines', 'last line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pTZex_7iBB3m"
   },
   "outputs": [],
   "source": [
    "lines = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[6] at readRDDFromFile at PythonRDD.scala:289\n"
     ]
    }
   ],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nHYySyTvBB3m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['line 1', '2', 'more lines', 'last line']\n"
     ]
    }
   ],
   "source": [
    "print(lines.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the length of each line in the RDD and store results in a new RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XltEtVriBB3m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1, 10, 9]\n"
     ]
    }
   ],
   "source": [
    "lineLengths = lines.map(lambda line: len(line))\n",
    "print(lineLengths.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum the lenght of lines in the RDD.  As RDD is partitioned, this reduce operation performs in a parallel fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NIolBGhSBB3n",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "totalLength = lineLengths.reduce(lambda a, b: a+b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2xTyLVwBB3n"
   },
   "outputs": [],
   "source": [
    "data = (1,2,3,4)\n",
    "rdd = sc.parallelize(data)\n",
    "rdd2 = rdd.map(lambda x: x*2)\n",
    "print(rdd2.collect())\n",
    "sum_val = rdd2.reduce(lambda a, b: a+b)\n",
    "print('sum = {0}'.format(sum_val))\n",
    "mul_val = rdd2.reduce(lambda a, b: a*b)\n",
    "print('mul = {0}'.format(mul_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCN2Ce92BB3n"
   },
   "source": [
    "## RDD Operations - aggregate\n",
    "\n",
    "Aggregate is an action operation *rdd.aggregate(zeroValue, seqOp, combOp)* that:\n",
    "- performs *seqOp* to *zeroValue* and all RDD elements -- this basically transforms all elements in RDD into the type of output value\n",
    "- and then aggregates the transformed RDD elements using *combOp*\n",
    "\n",
    "Note that reduce is a simple form of aggreate operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following aggregate operation is basically a *rdd.reduce(lambda a, b: a+b)* as the type output value is an integer which is the same as the RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.aggregate(0,\n",
    "              lambda zero, e: zero+e, \n",
    "              lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.aggregate(0,\n",
    "              lambda zero, e: zero+1, \n",
    "              lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following aggregate operation returns an order pairs of (x, y) where\n",
    "- x is the sum of all elements in RDD\n",
    "- y is the count of all elements in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLiDl_PeBB3n"
   },
   "outputs": [],
   "source": [
    "rdd.aggregate((0, 0),\n",
    "              lambda zero, e: (zero[0]+e, zero[1]+1), \n",
    "              lambda a, b: (a[0]+b[0], a[1]+b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following aggregate operation returns an order pairs of (x, y) where\n",
    "- x is the concatenation of all elements in RDD\n",
    "- y is the sum of the length of all elements in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlqSrrGsBB3n"
   },
   "outputs": [],
   "source": [
    "lines.aggregate((\"\", 0),\n",
    "                lambda zero, e: (zero[0]+e, zero[1]+len(e)),\n",
    "                lambda a, b: (a[0]+b[0], a[1]+b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.reduce(lambda s1, s2: s1+s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFTEFyIYBB3o"
   },
   "source": [
    "# Working with Text\n",
    "\n",
    "Before running this example, make sure that a data file 'star-wars.txt' has been uploaded to content folder of this colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read the content of the file using sc.textFile().  This creates an rdd whose elements are lines in the input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wuBT3HAwBB3o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: []\n",
      "35: [                          STAR WARS]\n",
      "41: [                    !! PUBLIC  VERSION !!]\n",
      "2: [  ]\n",
      "49: [          �A long time ago, in a galaxy far, far ]\n",
      "18: [          away...�]\n",
      "0: []\n",
      "55: [A vast sea of stars serves as the backdrop for the main]\n",
      "55: [title.  War drums echo through the heavens as a rollup ]\n",
      "28: [slowly crawls into infinity.]\n"
     ]
    }
   ],
   "source": [
    "sw = sc.textFile('star-wars.txt')\n",
    "for line in sw.take(10):\n",
    "    print('{0}: [{1}]'.format(len(line), line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wuBT3HAwBB3o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 7518 lines\n"
     ]
    }
   ],
   "source": [
    "print('Total = {0} lines'.format(sw.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all blank lines and lower all characters in all lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YB1lgr6kBB3p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non blank line = 4754 lines\n",
      "35: [                          star wars]\n",
      "41: [                    !! public  version !!]\n",
      "2: [  ]\n",
      "49: [          �a long time ago, in a galaxy far, far ]\n",
      "18: [          away...�]\n",
      "55: [a vast sea of stars serves as the backdrop for the main]\n",
      "55: [title.  war drums echo through the heavens as a rollup ]\n",
      "28: [slowly crawls into infinity.]\n",
      "47: [          �it is a period of civil war.  rebel ]\n",
      "45: [          spaceships, striking from a hidden ]\n"
     ]
    }
   ],
   "source": [
    "nb_lines = sw.filter(lambda line: len(line) > 0)\n",
    "print('Non blank line = {0} lines'.format(nb_lines.count()))\n",
    "all_lowers = nb_lines.map(lambda line: line.lower())\n",
    "for line in all_lowers.take(10):\n",
    "    print('{0}: [{1}]'.format(len(line), line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split each line into words.  Note that if we use *map* each element in the output RDD from *map* is a list of words in each line.  However, if we use *flatMap* lists in all lines are combined into an RDD of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['star', 'wars']\n",
      "['!!', 'public', 'version', '!!']\n",
      "[]\n",
      "['�a', 'long', 'time', 'ago,', 'in', 'a', 'galaxy', 'far,', 'far']\n",
      "['away...�']\n"
     ]
    }
   ],
   "source": [
    "words_map = all_lowers.map(lambda line: line.split())\n",
    "for l in words_map.take(5):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YYz4VHwsBB3p",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star\n",
      "wars\n",
      "!!\n",
      "public\n",
      "version\n",
      "!!\n",
      "�a\n",
      "long\n",
      "time\n",
      "ago,\n"
     ]
    }
   ],
   "source": [
    "words = all_lowers.flatMap(lambda line: line.split())\n",
    "for w in words.take(10):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the occurances of each word, we first transform a word into a pairwise (key, value) of (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('star', 1), ('wars', 1), ('!!', 1), ('public', 1), ('version', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: (word, 1)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transformation, we can count the occurances using *reduceByKey* which perform reduce(function) for all elements with the same key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "__qcouaNBB3p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('star', 211)\n",
      "('wars', 1)\n",
      "('!!', 2)\n",
      "('public', 1)\n",
      "('version', 1)\n",
      "('�a', 1)\n",
      "('long', 31)\n",
      "('time', 16)\n",
      "('ago,', 1)\n",
      "('in', 396)\n"
     ]
    }
   ],
   "source": [
    "mappers = words.map(lambda word: (word, 1))\n",
    "counts = mappers.reduceByKey(lambda x, y: x+y)\n",
    "for wc in counts.take(10):\n",
    "    print(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxN0eO63BB3p"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1 - Basic Spark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
